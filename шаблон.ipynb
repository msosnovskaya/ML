{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rc, plot\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, \n",
    "GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split,\n",
    "KFold, StratifiedKFold, cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import precision_recall_curve, \n",
    "classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv(\"train.csv\")\n",
    "n=train_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=pd.read_csv('test.csv')\n",
    "ID_test = test_df.PassengerId # запоминаем id \n",
    "y = train_df[\"(target class)\"]\n",
    "df=pd.concat([train_df,test_df]).drop([\"(target class)\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('балланс классов:',train_df['(target class)'].value_counts()) # баланс классов\n",
    "print('кол-во каждого категориального признака:',\n",
    "      df.dtypes.value_counts()) # Number of each type of column\n",
    "print('кол-во уникальных экземпляров катег. признаков:',\n",
    "      df.select_dtypes('object').apply(pd.Series.nunique, axis = 0)) # Number of unique classes in each object column\n",
    "print('статистики:',df[''].describe()) # хотим вычислить аномалии. аномалию сделать Nan,потом обработать как пропуск\n",
    "df[''].replace({365243: np.nan}, inplace = True) # Replace the anomalous values with nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingdata(data):  # пропущенные значения\n",
    "    total = data.isnull().sum().sort_values(ascending = False)\n",
    "    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n",
    "    ms=pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    ms= ms[ms[\"Percent\"] > 0]\n",
    "#     f,ax =plt.subplots(figsize=(8,6))\n",
    "#     plt.xticks(rotation='90')\n",
    "#     fig=sns.barplot(ms.index, ms[\"Percent\"],color=\"green\",alpha=0.8)\n",
    "#     plt.xlabel('Features', fontsize=15)\n",
    "#     plt.ylabel('Percent of missing values', fontsize=15)\n",
    "#     plt.title('Percent missing data by feature', fontsize=15)\n",
    "    return ms\n",
    "print('пропущенные значения')\n",
    "missingdata(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[''].fillna(df[''].mode()[0],\n",
    "              inplace = True) # заполняем пропуски категориальные\n",
    "df[''].fillna(df[''].median(), \n",
    "              inplace = True) # заполняем пропуски числовые\n",
    "drop_column = [''] # выбираем признаки которые удалим (лучше не удалять сначала)\n",
    "df.drop(drop_column, axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Визуализация признаков\n",
    "## Числовые (количественные признаки)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.[''].hist(); # гистограмма (плотность распределения)\n",
    "df.hist(); # гистограмма для всех числовых сразу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df['']); # ящик с усами. точки - выбросы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Категориальные и бинарные признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.[''].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df['']);\n",
    "# для первых пяти самых популярных\n",
    "sns.countplot(df[df[''].\\\n",
    "                 isin(df['']value_counts().head().index)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Взаимодействия\n",
    "## Числовые - числовые"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отбираем все признаки в которыхесть слово charge\n",
    "feat = [f for f in df.columns if 'charge' in f] \n",
    "feat\n",
    "df[feat].hist(); # строим гистограммы для всех  charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# матрица для нескольких признаков попарно (для небольшого числа!!!)\n",
    "sns.pairplot(df[feat]); # вне диагонали - диаграммы рассеяни"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для двух признаков\n",
    "plt.scatter(df['признак 1'],df['признак 2'],\n",
    "           color=df['target'].map({'0': 'green','1': 'red'})); \n",
    "plt.xlabel('название оси х $a^2$')\n",
    "plt.ylabel('название оси y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('корреляция\\n')  # признаки с высокой корреляцией можно удалить\n",
    "train_df.corrwith(train_df['Survived'],\n",
    "                  method='spearman').sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Числовой - Категориальный (Бинарный)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='категориальный (и target)', \n",
    "            y='числовой признак', data=df); # violinplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('категориальный(бинарный)')['числовой'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Категориальный- Категориальный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='категориальный', hue='targer', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для бустинговых (lgbm, catboost) надо просто выдетлить\n",
    "#категориальные признаки, для остальных: перекодировать\n",
    "\n",
    "le = LabelEncoder() # Create a label encoder object (2 варианта признака)\n",
    "le_count = 0\n",
    "for col in df:\n",
    "    if df[col].dtype == 'object':\n",
    "        # If 2 or fewer unique categories\n",
    "        if len(list(df[col].unique())) <= 2:\n",
    "            # Train on the training data\n",
    "            le.fit(df[col])\n",
    "            # Transform both training and testing data\n",
    "            df[col] = le.transform(df[col])\n",
    "            le_count += 1\n",
    "print('%d columns were label encoded.' % le_count)\n",
    "\n",
    "df = pd.get_dummies(df) # one hot encoder (более 2 вариантов признака)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = train_df.select_dtypes(['int64', 'float64']).corr()\n",
    "plt.figure(figsize=(20, 20))\n",
    "ax=sns.heatmap(corr_matrix)\n",
    "\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_fontsize(8)\n",
    "for tick in ax.get_yticklabels():\n",
    "    tick.set_fontsize(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "перед созданием bins можно вызвать метод describe (смотреть квантили), чтобы было легче разбивать на группы (концы не включаются!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age_bin'] = pd.cut(df['Age'], bins=[0,14,20,40,120], \n",
    "                       labels=['Children','Teenage','Adult','Elder'])\n",
    "df['Fare_bin'] = pd.cut(df['Fare'], bins=[0,7.91,14.45,31,120],\n",
    "                        labels=['Low_fare','median_fare', 'Average_fare',\n",
    "                                'high_fare'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df\n",
    "dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "import re # для работы с регулярными выражениями, например поиск в строке\n",
    "# Define function to extract titles from passenger names\n",
    "def get_title(name):\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "# Create a new feature Title, containing the titles of passenger names\n",
    "dataset['Title'] = dataset['Name'].apply(get_title)\n",
    "# Group all non-common titles into one single grouping \"Rare\"\n",
    "dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess',\n",
    "                                             'Capt', 'Col','Don',\n",
    "                                             'Dr', 'Major', 'Rev', \n",
    "                                             'Sir', 'Jonkheer', \n",
    "                                             'Dona'], 'Rare')\n",
    "dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "df=dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.preprocessing import scale # анализ главных компонент \n",
    "X_scaled = scale(X.iloc[:,:5].dropna(), axis=0) # было 5 стоблцов \n",
    "from sklearn.decomposition import PCA # мз 5 мерного перешли в 2 мерное и сохранили 70 процентов дисперсии\n",
    "pca = PCA(n_components=2, random_state=42) # стало 2 стоблцов\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new dataframe for polynomial features\n",
    "poly_features = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', \n",
    "                    'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\n",
    "poly_target = poly_features['TARGET']\n",
    "poly_features = poly_features.drop(columns = ['TARGET'])\n",
    "# Need to impute missing values\n",
    "poly_features = imputer.fit_transform(poly_features)\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_transformer = PolynomialFeatures(degree = 3)\n",
    "poly_transformer.fit(poly_features)\n",
    "# Transform the features\n",
    "poly_features = poly_transformer.transform(poly_features)\n",
    "print('Polynomial Features shape: ', poly_features.shape)\n",
    "# Merge polynomial features into training dataframe\n",
    "poly_features['SK_ID_CURR'] = df['SK_ID_CURR']\n",
    "df_poly = df.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n",
    "# Align the dataframes\n",
    "df_poly= df_poly.align(app_test_poly, join = 'inner', axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=df.iloc[n:,:] #разделить трейн и тест перед обучением\n",
    "X=df.iloc[:n,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "tree_params = {'max_depth': np.arange(1,11),\n",
    "              'max_features':[0.5,0.7,1]}\n",
    "\n",
    "tree_grid=GridSearchCV(first_tree,tree_params,cv=5,n_jobs=4)\n",
    "tree_grid.fit(X_train,y_train)\n",
    "tree_grid.best_score_\n",
    "tree_grid.best_params_\n",
    "tree_grid.best_estimator_ # лучшая модель \n",
    "tree_test_pred=tree.grid.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,tree_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import   #for score!!!!!!!!!!!!!!!\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,\n",
    "                                                 test_size=0.3,\n",
    "                                                 random_state=42)\n",
    "# стратифмцировать!!!\n",
    "kf = StratifiedKFold(n_splits=5, random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Make the model with the specified regularization parameter\n",
    "log_reg = LogisticRegression(C = 0.0001)\n",
    "\n",
    "# Train on the training data\n",
    "log_reg.fit(train, train_labels)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "rfc.fit(X_train,y_train)\n",
    "y_pred=rfc.predict(X_test)\n",
    "average_precision = average_precision_score(y_test, y_pred) # задать скор!!!!!!!!!!!!!!\n",
    "print('score: {0:0.2f}'.format(\n",
    "    average_precision))\n",
    "cv_scores = cross_val_score(rfc, X_train, y_train, cv=kf, scoring='average_precision') # задать скор!!!!!!!!!!!!!!\n",
    "#print(\"Cross validation scores:\\n\\t\", \"\\n\\t\".join(\"%.4f\" % x for x in cv_scores))\n",
    "print(\" CV score = %.4f\" % np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = list(X.select_dtypes(include='object')) #катбусту нужно сообщать о категориальных признаках, это их запоминает\n",
    "best_model.fit(\n",
    "   X_train, y_train,\n",
    "   cat_features=cat_col,\n",
    "   eval_set=(X_eval, y_eval),\n",
    "   plot=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from catboost import CatBoostClassifier, cv, Pool\n",
    "cat=CatBoostClassifier(random_state=42, eval_metric='BalancedAccuracy')\n",
    "cat.fit(X_train,y_train, verbose=False, plot = True,eval_set=(X_test,y_test))\n",
    "y_pred1=cat.predict(X_test)\n",
    "average_precision = average_precision_score(y_test, y_pred1)\n",
    "print('Average precision-recall score (cat): {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "result=cross_val_score(cat,y,Targeted_feature,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for CatBoostClassifier is:',round(result.mean()*100,2))\n",
    "y_predict = cross_val_predict(cat,train_df,y,cv=kf)\n",
    "# задать сv у catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb  # разобраться \n",
    "params = {\n",
    "    'objective' :'binary',\n",
    "    'learning_rate' : 0.02,\n",
    "    'num_leaves' : 76,\n",
    "    'feature_fraction': 0.64, \n",
    "    'bagging_fraction': 0.8, \n",
    "    'bagging_freq':1,\n",
    "    'boosting_type' : 'gbdt',\n",
    "    'metric': 'binary_logloss'\n",
    "}\n",
    " # making lgbm datasets for train and valid\n",
    "    d_train = lgbm.Dataset(X_train, y_train)\n",
    "    d_valid = lgbm.Dataset(X_test, y_test)\n",
    "    \n",
    "gbm = lgb.train(params, d_train, 5000, valid_sets=[d_valid], verbose_eval=50, early_stopping_rounds=100,feval=binary_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = pd.DataFrame(model.predict(test_df),columns=['(target class)']) \n",
    "ans.index = ID_test\n",
    "ans.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
